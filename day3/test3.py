import pandas as pd
import time
import random
import os
from pathlib import Path
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.chrome.options import Options
from fake_useragent import UserAgent

# å¾…çˆ¬å–çš„æ–‡ç« åˆ—è¡¨
paper_titles = [
    "Automatic crater detection and age estimation for mare regions on the lunar surface",
    "The origin of planetary impactors in the inner solar system",
    "Deep learning based systems for crater detection: A review",
    "A preliminary study of classification method on lunar topography and landforms",
    "The CosmoQuest Moon mappers community science project: The effect of incidence angle on the Lunar surface crater distribution",
    "Fast r-cnn",
    "You only look once: Unified, real-time object detection",
    "Attention is all you need",
    "End-to-end object detection with transformers"
]

# åˆ›å»ºç›®æ ‡ç›®å½•
save_dir = r"D:\shixun\è®ºæ–‡çˆ¬å–"
os.makedirs(save_dir, exist_ok=True)

# åˆå§‹åŒ–ç”¨æˆ·ä»£ç†ç”Ÿæˆå™¨
ua = UserAgent()


def get_random_headers():
    """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': random.choice(['en-US,en;q=0.9', 'zh-CN,zh;q=0.9', 'ja,en-US;q=0.9,en;q=0.8']),
        'Referer': 'https://scholar.google.com/'
    }


def human_like_delay(min_delay=2.0, max_delay=8.0):
    """æ¨¡æ‹Ÿäººç±»æ“ä½œé—´éš”ï¼ˆæ™ºèƒ½å»¶æ—¶ï¼‰"""
    delay = random.uniform(min_delay, max_delay)
    print(f"â³ æ¨¡æ‹Ÿæ€è€ƒï¼Œç­‰å¾… {delay:.1f} ç§’...")
    time.sleep(delay)
    return delay


def simulate_human_typing(element, text):
    """æ¨¡æ‹Ÿäººç±»æ‰“å­—è¡Œä¸º"""
    actions = ActionChains(driver)
    for char in text:
        actions.send_keys_to_element(element, char)
        actions.perform()
        time.sleep(random.uniform(0.05, 0.3))  # éšæœºæŒ‰é”®é—´éš”


def scroll_randomly():
    """éšæœºæ»šåŠ¨é¡µé¢æ¨¡æ‹Ÿäººç±»æµè§ˆ"""
    scroll_height = driver.execute_script("return document.body.scrollHeight")
    scroll_times = random.randint(2, 5)

    for _ in range(scroll_times):
        # éšæœºæ»šåŠ¨ä½ç½®
        scroll_pos = random.randint(200, int(scroll_height * 0.8))
        driver.execute_script(f"window.scrollTo(0, {scroll_pos});")

        # éšæœºç­‰å¾…æ—¶é—´
        time.sleep(random.uniform(0.5, 2.5))

        # éšæœºé¼ æ ‡ç§»åŠ¨
        x_offset = random.randint(-100, 100)
        y_offset = random.randint(-50, 50)
        ActionChains(driver).move_by_offset(x_offset, y_offset).perform()
        ActionChains(driver).move_by_offset(-x_offset, -y_offset).perform()


def setup_driver():
    """é…ç½®æµè§ˆå™¨é©±åŠ¨"""
    chrome_options = Options()

    # è®¾ç½®éšæœºUser-Agent
    user_agent = ua.random
    chrome_options.add_argument(f'user-agent={user_agent}')

    # å¯ç”¨æ— å¤´æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    # chrome_options.add_argument('--headless')

    # ç¦ç”¨è‡ªåŠ¨åŒ–ç‰¹å¾
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    # ç¦ç”¨å›¾ç‰‡åŠ è½½åŠ é€Ÿ
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)

    # åˆå§‹åŒ–æµè§ˆå™¨
    driver = webdriver.Chrome(options=chrome_options)

    # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    return driver


def fetch_through_browser():
    """ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿäººç±»æ“ä½œè·å–æ–‡çŒ®æ•°æ®"""
    results = []
    global  driver
    driver = setup_driver()

    try:
        # æ‰“å¼€Google Scholar
        driver.get("https://scholar.google.com")
        human_like_delay(3, 6)  # åˆå§‹é¡µé¢åŠ è½½ç­‰å¾…

        for i, title in enumerate(paper_titles):
            try:
                print(f"\nğŸ” å¼€å§‹å¤„ç†: {title} ({i + 1}/{len(paper_titles)})")

                # å®šä½æœç´¢æ¡†
                search_box = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.NAME, "q"))
                )

                # æ¸…é™¤æœç´¢æ¡†
                search_box.clear()
                human_like_delay(0.5, 1.5)

                # æ¨¡æ‹Ÿäººç±»è¾“å…¥
                simulate_human_typing(search_box, title)
                human_like_delay(1, 2)  # è¾“å…¥åçŸ­æš‚åœé¡¿

                # æäº¤æœç´¢
                search_box.submit()

                # ç­‰å¾…ç»“æœåŠ è½½
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.ID, "gs_res_ccl"))
                )

                # æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º
                scroll_randomly()

                # è·å–ç¬¬ä¸€æ¡ç»“æœ
                try:
                    first_result = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".gs_r.gs_or.gs_scl"))
                    )

                    # æå–ä¿¡æ¯
                    result = extract_result_info(first_result, title)
                    results.append(result)
                    print(f"âœ… æˆåŠŸè·å–: {title}")

                except Exception as e:
                    print(f"âŒ ç»“æœè§£æå¤±è´¥: {str(e)}")
                    results.append({'title': title, 'error': 'ç»“æœè§£æå¤±è´¥'})

                # éšæœºå»¶æ—¶åè¿”å›é¦–é¡µ
                human_like_delay(3, 8)
                driver.get("https://scholar.google.com")
                human_like_delay(2, 4)

            except Exception as e:
                print(f"ğŸ”¥ å¤„ç†å¤±è´¥ [{title}]: {str(e)}")
                results.append({'title': title, 'error': str(e)})

                # é”™è¯¯åå¢åŠ é¢å¤–å»¶æ—¶
                time.sleep(random.uniform(8, 15))

                # åˆ·æ–°æµè§ˆå™¨
                driver.refresh()
                human_like_delay(3, 6)

    finally:
        # å…³é—­æµè§ˆå™¨
        driver.quit()

    return results


def extract_result_info(result_element, original_title):
    """ä»ç»“æœå…ƒç´ ä¸­æå–ä¿¡æ¯"""
    # æ ‡é¢˜
    title_elem = result_element.find_element(By.CSS_SELECTOR, "h3 a")
    title = title_elem.text

    # ä½œè€…å’Œå¹´ä»½
    authors_year = result_element.find_element(By.CSS_SELECTOR, ".gs_a").text
    parts = authors_year.split("-")
    authors = parts[0].strip() if len(parts) > 0 else "N/A"
    year = parts[1].strip() if len(parts) > 1 else "N/A"

    # å¼•ç”¨é‡
    citations = "0"
    try:
        citations_elem = result_element.find_element(By.CSS_SELECTOR, ".gs_ri .gs_fl a:nth-child(3)")
        if "Cited by" in citations_elem.text:
            citations = citations_elem.text.split("Cited by ")[1]
    except:
        pass

    # æ‘˜è¦ç‰‡æ®µ
    abstract = "N/A"
    try:
        abstract_elem = result_element.find_element(By.CSS_SELECTOR, ".gs_rs")
        abstract = abstract_elem.text[:200] + '...'
    except:
        pass

    # ç›¸ä¼¼åº¦æ£€æŸ¥
    similarity = calculate_title_similarity(original_title, title)

    return {
        'title': title,
        'original_title': original_title,
        'similarity': f"{similarity:.1f}%",
        'authors': authors,
        'year': year,
        'citations': citations,
        'abstract': abstract,
        'source': 'selenium'
    }


def calculate_title_similarity(title1, title2):
    """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
    # ç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
    set1 = set(title1.lower().split())
    set2 = set(title2.lower().split())
    intersection = set1 & set2
    union = set1 | set2
    return (len(intersection) / len(union)) * 100 if union else 0


# ä¸»æ‰§è¡Œé€»è¾‘
if __name__ == "__main__":
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿæ–¹æ¡ˆ
    scholarly_data = fetch_through_browser()

    # ä¿å­˜ç»“æœåˆ°Excel
    save_path = Path(save_dir) / "scholar_results.xlsx"
    df = pd.DataFrame(scholarly_data)

    try:
        df.to_excel(save_path, index=False)
        print(f"\nğŸ‰ æ•°æ®å·²ä¿å­˜è‡³: {save_path}")
    except Exception as e:
        print(f"\nâŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}")
        # å°è¯•åˆ›å»ºçˆ¶ç›®å½•åé‡è¯•
        os.makedirs(save_path.parent, exist_ok=True)
        df.to_excel(save_path, index=False)
        print(f"âœ… é‡è¯•åä¿å­˜æˆåŠŸ: {save_path}")

    # è®¡ç®—æ€»è€—æ—¶
    total_time = time.time() - start_time
    mins, secs = divmod(total_time, 60)

    # æ·»åŠ å®Œæˆæç¤º
    print("\n" + "=" * 50)
    print(f"ä»»åŠ¡å®Œæˆ! å…±å¤„ç† {len(paper_titles)} ç¯‡è®ºæ–‡")
    success_count = len([x for x in scholarly_data if 'error' not in x])
    failed_count = len(paper_titles) - success_count
    print(f"æˆåŠŸè·å–: {success_count} ç¯‡")
    print(f"è·å–å¤±è´¥: {failed_count} ç¯‡")
    print(f"æ€»è€—æ—¶: {int(mins)}åˆ†{int(secs)}ç§’")
    print(f"å¹³å‡æ¯ç¯‡è€—æ—¶: {total_time / len(paper_titles):.1f}ç§’")
    print("=" * 50)

    # ä¿å­˜è¯¦ç»†æ—¥å¿—
    log_path = Path(save_dir) / "crawl_log.json"
    with open(log_path, 'w') as f:
        json.dump({
            "date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "papers_processed": len(paper_titles),
            "success_count": success_count,
            "failed_count": failed_count,
            "total_time": total_time
        }, f, indent=2)
    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜è‡³: {log_path}")