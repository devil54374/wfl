import requests
from lxml import etree
import os
import time
import random
from urllib.parse import urljoin


def download_images():
    # å¢å¼ºåçˆ¬é…ç½®
    url = "http://pic.netbian.com/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': url,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Connection': 'keep-alive'
    }
    save_dir = r"D:\shixun\å›¾ç‰‡çˆ¬å–"  # åŸå§‹å­—ç¬¦ä¸²ä¿®å¤è·¯å¾„é—®é¢˜

    # è‡ªåŠ¨åˆ›å»ºç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    try:
        # è·å–ç½‘é¡µæºç ï¼ˆå¸¦è¶…æ—¶é‡è¯•ï¼‰
        for _ in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
            try:
                response = requests.get(url, headers=headers, timeout=15)
                response.encoding = 'gbk'
                response.raise_for_status()
                break
            except requests.exceptions.RequestException:
                time.sleep(random.uniform(2, 5))  # æŒ‡æ•°é€€é¿é‡è¯•
        else:
            print("âŒ å¤šæ¬¡è¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return

        # è°ƒè¯•ï¼šä¿å­˜ç½‘é¡µæºç ï¼ˆå¯é€‰ï¼‰
        with open(os.path.join(save_dir, "page_source.html"), "w", encoding="gbk") as f:
            f.write(response.text)

        # è§£æHTMLï¼ˆä½¿ç”¨å¤‡ç”¨XPathç­–ç•¥ï¼‰
        html = etree.HTML(response.text)

        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨æ›´é€šç”¨çš„å›¾ç‰‡é€‰æ‹©å™¨
        img_elements = html.xpath("//img[@src and contains(@src, 'uploads')]")

        # æ–¹æ¡ˆ2ï¼šå¦‚æœæ–¹æ¡ˆ1å¤±è´¥ï¼Œå°è¯•åŒ¹é…å›¾ç‰‡å®¹å™¨
        if not img_elements:
            img_elements = html.xpath("//div[contains(@class, 'list')]//img")

        # æ–¹æ¡ˆ3ï¼šç»ˆæå›é€€æ–¹æ¡ˆ
        if not img_elements:
            img_elements = html.xpath("//img[@src]")
            print("âš ï¸ ä½¿ç”¨é€šç”¨é€‰æ‹©å™¨ï¼Œå¯èƒ½åŒ…å«éç›®æ ‡å›¾ç‰‡")

        if not img_elements:
            print("â›” æœªæ‰¾åˆ°å›¾ç‰‡å…ƒç´ ï¼å¯èƒ½åŸå› ï¼š")
            print("- ç½‘ç«™ç»“æ„å·²æ›´æ–° [6](@ref)")
            print("- è§¦å‘åçˆ¬æœºåˆ¶ [5](@ref)")
            print("- åŠ¨æ€åŠ è½½å†…å®¹ï¼ˆéœ€æ”¹ç”¨Seleniumï¼‰")
            return

        print(f"âœ… å‘ç°{len(img_elements)}å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")

        # ä¸‹è½½æ¯å¼ å›¾ç‰‡
        for i, img in enumerate(img_elements, 1):
            # è·å–ç›¸å¯¹è·¯å¾„å¹¶ä¿®å¤ç‰¹æ®Šå­—ç¬¦
            relative_path = img.xpath('./@src')[0].replace('\\', '/')
            full_url = urljoin(url, relative_path)

            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            file_name = f"{i}_{os.path.basename(full_url).split('?')[0]}"
            save_path = os.path.join(save_dir, file_name)

            try:
                # å›¾ç‰‡è¯·æ±‚æ·»åŠ Referer
                img_headers = headers.copy()
                img_headers['Referer'] = full_url

                img_response = requests.get(full_url, headers=img_headers, timeout=20)
                img_response.raise_for_status()

                # éªŒè¯å›¾ç‰‡æœ‰æ•ˆæ€§
                if len(img_response.content) < 1024:
                    print(f"âš ï¸ å›¾ç‰‡è¿‡å°å¯èƒ½æ— æ•ˆ: {file_name}")
                    continue

                with open(save_path, 'wb') as f:
                    f.write(img_response.content)
                print(f"âœ… [{i}/{len(img_elements)}] å·²ä¿å­˜: {file_name}")

                # æ™ºèƒ½å»¶æ—¶ï¼ˆæ ¹æ®å›¾ç‰‡å¤§å°è°ƒæ•´ï¼‰
                delay = max(1, len(img_response.content) / (1024 * 50))  # æ¯50KBå»¶è¿Ÿ1ç§’
                time.sleep(delay + random.uniform(0.5, 1.5))

            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {file_name} | é”™è¯¯: {str(e)}")

    except Exception as e:
        print(f"ğŸ”¥ å…¨å±€å¼‚å¸¸: {str(e)}")


if __name__ == "__main__":
    download_images()
    print("=" * 50)
    print(f"ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼ä¿å­˜ç›®å½•: {r'D:\shixun\å›¾ç‰‡çˆ¬å–'}")